**Text Cleaning and Setup**

```{python}
import spacy
from nltk.corpus import wordnet as wn
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from spacy.matcher import PhraseMatcher
from spacy_wordnet.wordnet_annotator import WordnetAnnotator

def get_unique_tokens(tokens):
    unique_tokens = []
    for token in tokens:
        if token.lower() in unique_tokens:
            continue
        unique_tokens.append(token.lower())
    return unique_tokens

def get_unique_lemmas(lemmas):
    counter = Counter(lemmas)
    data = [(l, counter[l]) for l in lemmas]
    data_unique = []
    unique_lemmas = []
    for item in data:
        if item[0].lower() in unique_lemmas:
            continue
        unique_lemmas.append(item[0].lower())
        data_unique.append(item)
    return data_unique

#nltk.download('wordnet')
nlp_en = spacy.load("en_core_web_sm")
print("English loaded!")
nlp_fr = spacy.load("fr_core_news_sm") # No pipeline 'Tagger'
print("French loaded!")
nlp_es = spacy.load("es_core_news_sm") # No pipeline 'Tagger'
print("Spanish loaded!")
nlp_it = spacy.load("it_core_news_sm")
print("Italian loaded!")

nlp_en.add_pipe("spacy_wordnet", after="tagger")
nlp_it.add_pipe("spacy_wordnet", after="tagger")

with open("data/english.txt", "r", encoding="utf-8") as f:
    text_en = f.read()
with open("data/english.txt", "r", encoding="utf-8") as f:
    text_fr = f.read()
with open("data/english.txt", "r", encoding="utf-8") as f:
    text_es = f.read()
with open("data/english.txt", "r", encoding="utf-8") as f:
    text_it = f.read()

# type-token ratio :: The term types refers to the number of distinct words. The term tokens refers to the number of all words.
def clean(lang, text, nlp):
    doc = nlp(text)
    doc = [t for t in doc
            if (not (t.is_punct or t.is_space or t.like_num)) and t.is_alpha]
    # Must check for sentence-starts here in case stop word is start of sentence
    sentence_starts = [t for t in doc if t.is_sent_start]
    print(sentence_starts)
    print(len(sentence_starts))
    doc = [t for t in doc if not t.is_stop]

    tokens = [t.text for t in doc]
    lemmas = [t.lemma_.lower() for t in doc]

    unique_lemmas = get_unique_lemmas(lemmas) # [(lemma, count)...]
    unique_tokens = get_unique_tokens(tokens)

    unique_tokens_count = len(unique_tokens)

    total_tokens = len([t.text for t in doc])
    unique_lemmas_count = len(unique_lemmas)
    lexical_diversity = unique_tokens_count/float(total_tokens)

clean("English", text_en, nlp_en)

doc = nlp_en(text_en)
doc = [t for t in doc
        if not (t.is_stop or t.is_punct or t.is_space or t.like_num)]

freq = Counter(content_lemmas)
print(freq.items())
print(lemma_table(doc, freq))
sentence_df = pd.DataFrame(freq.items(), columns=["lemma", "count"]).sort_values("count", ascending=False)
print(sentence_df.head())

df = pd.DataFrame.from_dict(dicct, orient="index", columns=["Total Tokens", "Unique Lemmas", "Lexical Diversity", "Number of Sentences"])
```

**Frequency and Collocations**

```{python}

```

**Syntax Patterns**

```{python}

```

**Semantics and WordNet**

```{python}

```
